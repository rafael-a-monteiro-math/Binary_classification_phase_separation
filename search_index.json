[
["a-few-examples.html", " 2 Nonlinear diffusion equations: a numerical example 2.1 Nonlinear diffusion equations: some illustrative examples 2.2 Propagation with randomly generated coefficients", " 2 Nonlinear diffusion equations: a numerical example To begin with, I will introduce the model, giving some examples of its use. Let’s first import some libraries from sklearn.model_selection import train_test_split, StratifiedKFold from sklearn.preprocessing import MinMaxScaler import matplotlib.gridspec as gridspec ... 2.1 Nonlinear diffusion equations: some illustrative examples As discussed in Section 1.1 in the paper, the foundations of the model lie on nonlinear diffusion processes, which we briefly illustrate with an example. The heart of the model is the Allen-Cahn equation (Fife 1979),(Aronson and Weinberger 1978),(Allen and Cahn 1979), a well-known equation in the field of pattern formation. Just to show how the code we have can be used in that case, we will plot the evolution of an initial boundary value problem, with Neumann boundary conditions. Let’s see first take a look at the evolution of \\[\\begin{equation} u_0(x) = \\frac{1- \\sin(\\pi(2x - 1))}{2}\\tag{2.1} \\end{equation}\\] as an initial condition to the Allen-Cahn equation \\[\\begin{equation} \\partial_tu(x, t) = \\varepsilon^2 \\partial_x^2u(x, t) + u(x, t)(1 - u(x, t))(u(x, t) - \\alpha(x)).\\tag{2.2} \\end{equation}\\] The parameter \\(\\alpha(\\cdot)\\) embodies medium heterogeneity. In this case, we choose \\(\\alpha(x) = -2\\), when \\(x &lt;0.5\\), and \\(\\alpha(x)\\) = 2, when \\(x \\geq 0.5\\). Parameters to the model are given below: N = 20 x = np.linspace(0, 1, N, endpoint = True) V_0 = 1/2 - 1/2 * np.reshape(np.sin(np.pi * (2 * x - 1)) , (-1,1)) prop = Propagate() dt, eps, Nx, Nt = 0.1, .3, N, 400 dx, ptt_cardnlty, weigths_k_sharing = x[1]-x[0], Nx, Nt Then we initialize parameters init = Initialize_parameters() param = init.dictionary(N, eps, dt, dx, Nt, ptt_cardnlty, weigths_k_sharing) If you read the paper you remember that trainable weights are the coefficients of this PDE. Since the model randomly initialize these coefficients, we will have to readjust them to the value we want. That’s what we do in the next part of the code. for i in range(param[&quot;Nt&quot;]): param[&quot;alpha_x_t&quot;][:,i] = -2 * (x &lt; .5) + 2 * (x &gt;= .5) which we now run, using the numerical scheme (1.7a) in the paper. As poijnted out there, this is the same as doing a forward propagation: that’s why you see the method “prop.forward” in the code below. flow, waterfall, time = prop.forward(V_0, param, waterfall_save = True , Flow_save = True) time = np.arange(Nt + 1) X, Y = np.meshgrid(x, time) flow = np.squeeze(flow, axis = 1) When we plot the evolution of \\(u_0(\\cdot)\\) through the Allen-Cahn equation as a surface, we get the plot below (code in Notebook_examples.ipynb). One of the main motivation of this project came after reading the interesting paper (Angenent, Mallet-Paret, and Peletier 1987): roughly speaking, they have shown that several nontrivial layered patterns^{Stationary solutions to equation (2.2) that, roughly speaking, gets “concentrated” around values 0 and 1, displaying layer in between these values.} can be found if \\(\\alpha(\\cdot)\\) is non-homogeneous in space. There is an extensive discussion about why this is interesting, and we refer the reader to Section 1.1 in the paper. Remark: You can read more about pattern formation in the book (Nishiura 2002) (Chapter 4.2 deals with the Allen-Cahn model), and also in the very nice article Arnd Scheel wrote to the Princeton Companion to Applied Mathematics, Section IV. 27 (Dennis et al. 2015). 2.2 Propagation with randomly generated coefficients Next, we would like to evolve and plot the evolution of several different initial conditions in the interval [0,1] (code in Notebook_examples.ipynb). The model that we use is, initially, a discretization of (2.2), with \\(\\varepsilon = 0\\), which then becomes an ODE: \\[\\begin{equation} U^{[n+1]} = U^{[n]} + \\Delta_t^{u}f(U^{[n]},\\alpha^{[n]}),\\tag{2.3} \\end{equation}\\] where \\(f(U^{[n]},\\alpha^{[n]}):= U^{[n]}(1 - U^{[n]})(U^{[n]} - \\alpha^{[n]} )\\). It is good to have in mind that the PDE coefficients will take the role of trainable weights in ML: we will “adjust” the coefficients in \\(\\alpha(\\cdot)\\) in order to achieve a certain final, end state. There is in fact a clear correspondence between the Initial value problem and forward propagation and, consequently, the stability of (2.3) has to be considered. The discretization it presents is known as (explicit) Euler method, and it is is known to be unstable in many cases. A good part of the paper was devoted to showing that there is some kind of nonlinear stabilization mechanism that prevents solutions from blowing up. This condition, referred in the paper as Invariant Region Enforcing Condition, shows that as long as we have some control on \\[\\begin{equation} \\max_{1\\leq k \\leq \\mathrm{N_t}}\\max\\{1,\\vert \\alpha^{[k]}\\vert\\},\\tag{2.4} \\end{equation}\\] we can adjust the parameter \\(\\Delta_t^u\\) in a nontrivial way^{Because, of course, \\(\\Delta_t^u =0\\) also does the job, but does not deliver what we want.} so that the evolution \\(U^{[\\cdot]}\\) does not yield a floating point overflow (in other words, a blow up in \\(\\ell^{\\infty}\\) norm). we set up a little experiment by taking several initial condition on the interval \\([0,1]\\) and evolving them according to (2.3). Parameters are as follows: N = 1 init = Initialize_parameters() prop = Propagate() dt_vec = np.array([.1,.3,.57,1.5,3,4]) dt, eps, Nx, Nt, dx = .1, 0, N, 20, 1 ptt_cardnlty, weights_k_sharing = Nx, Nt The model randomly initializes the coefficients as realizations of a Normal random variable with average 0.5 and variance 0.1, therefore, in light of (2.4), we reset them to gain more control on their boundedness as uniform random variables in the interval [0,1]. param = init.dictionary(N, eps, dt, dx, Nt, ptt_cardnlty, weights_k_sharing) for i in range(param[&quot;Nt&quot;]): param[&quot;alpha_x_t&quot;][:,i] = np.random.uniform(0,1) n_points = 10 V_0 = np.reshape(1/n_points * np.arange(0, n_points + 1), (1, -1)) flow, waterfall, time = prop.forward(V_0, param, waterfall_save = True , Flow_save = True) We plot several of these orbits with different values of \\(\\Delta_t^u\\) to illustrate the effectiveness of the Invariant Region Enforcing Condition, which settles a critical threshold for the size of \\(\\Delta_t^u\\), beyond which solutions can blow up. This is discussed at lenght in the paper. In short, the reasoning behind the existence of critical values of \\(\\mathrm{\\Delta_{t}^u}\\) under which the solution is “well behaved” (that is, the solution is always bounded) goes back to the idea of Invariant regions, exploited extensively in PDEs. It has been developed by many (take a look at Chapter 14 in (Smoller 1994)) and if you want to see how it applies in the discrete setting, especially in finite-difference schemes for reaction diffusion models, see (Hoff 1978) and the Appendix C of the paper). References "]
]
