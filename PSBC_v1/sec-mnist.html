<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title> 4 The MNIST database | Binary classification as a phase separation process - a short tutorial</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content=" 4 The MNIST database | Binary classification as a phase separation process - a short tutorial" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content=" 4 The MNIST database | Binary classification as a phase separation process - a short tutorial" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  



<meta name="date" content="2020-09-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-1d.html"/>
<link rel="next" href="the-phase-separation-binary-classifier-where-to-read-more-about-it.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Binary classification as a phase separation process</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="a-few-examples.html"><a href="a-few-examples.html"><i class="fa fa-check"></i><b>2</b> Nonlinear diffusion equations: a numerical example</a><ul>
<li class="chapter" data-level="2.1" data-path="a-few-examples.html"><a href="a-few-examples.html#propagation-with-randomly-generated-coefficients"><i class="fa fa-check"></i><b>2.1</b> Propagation with randomly generated coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="sec-1d.html"><a href="sec-1d.html"><i class="fa fa-check"></i><b>3</b> Applying the PSBC model to some toy problems</a><ul>
<li class="chapter" data-level="3.1" data-path="sec-1d.html"><a href="sec-1d.html#the-1d-rectangular-box-problem"><i class="fa fa-check"></i><b>3.1</b> The 1D Rectangular box problem</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sec-mnist.html"><a href="sec-mnist.html"><i class="fa fa-check"></i><b>4</b> The MNIST database</a><ul>
<li class="chapter" data-level="4.1" data-path="sec-mnist.html"><a href="sec-mnist.html#retrieving-some-statistics"><i class="fa fa-check"></i><b>4.1</b> Retrieving some statistics</a></li>
<li class="chapter" data-level="4.2" data-path="sec-mnist.html"><a href="sec-mnist.html#a-homemade-example-hadwritten-0-and-1"><i class="fa fa-check"></i><b>4.2</b> A “homemade” example: hadwritten 0 and 1</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-phase-separation-binary-classifier-where-to-read-more-about-it.html"><a href="the-phase-separation-binary-classifier-where-to-read-more-about-it.html"><i class="fa fa-check"></i><b>5</b> The Phase Separation Binary Classifier: where to read more about it</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
<li><a href="https://sites.google.com/view/rafaelmonteiro-math/home" target="blank">Rafael Monteiro's website</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Binary classification as a phase separation process - a short tutorial</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:mnist" class="section level1">
<h1><span class="header-section-number"> 4</span> The MNIST database</h1>
<p>The MNIST database is a well known database of handwritten digits used in the classical paper <span class="citation">(Lecun et al. <a href="#ref-Mnist" role="doc-biblioref">1998</a>)</span>. In this project we only used (suggestively) the subset of digits “0” and “1”, for we are doing binary classification only.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="sec-mnist.html#cb1-1"></a><span class="co">### READ MNIST DATASET TO PANDAS DATAFRAME AND THEN TO NUMPY FILE</span></span>
<span id="cb1-2"><a href="sec-mnist.html#cb1-2"></a>data_train_MNIST <span class="op">=</span> pd.read_csv(<span class="st">&#39;Examples/data_train_normalized_MNIST.csv&#39;</span>)</span>
<span id="cb1-3"><a href="sec-mnist.html#cb1-3"></a>data_test_MNIST <span class="op">=</span> pd.read_csv(<span class="st">&#39;Examples/data_test_normalized_MNIST.csv&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="sec-mnist.html#cb2-1"></a>X_train_MNIST <span class="op">=</span> (data_train_MNIST.iloc[:,:<span class="op">-</span><span class="dv">1</span>]).to_numpy()</span>
<span id="cb2-2"><a href="sec-mnist.html#cb2-2"></a>Y_train_MNIST <span class="op">=</span> np.reshape(data_train_MNIST.iloc[:,<span class="op">-</span><span class="dv">1</span>].to_numpy(), (<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb2-3"><a href="sec-mnist.html#cb2-3"></a>X_test_MNIST <span class="op">=</span> (data_test_MNIST.iloc[:,:<span class="op">-</span><span class="dv">1</span>]).to_numpy()</span>
<span id="cb2-4"><a href="sec-mnist.html#cb2-4"></a>Y_test_MNIST <span class="op">=</span> np.reshape(data_test_MNIST.iloc[:,<span class="op">-</span><span class="dv">1</span>].to_numpy(), (<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb2-5"><a href="sec-mnist.html#cb2-5"></a></span>
<span id="cb2-6"><a href="sec-mnist.html#cb2-6"></a>X_train_MNIST, X_test_MNIST <span class="op">=</span> X_train_MNIST.T , X_test_MNIST.T</span></code></pre></div>
<!-- ```python -->
<!-- print(X_test_MNIST.shape, Y_test_MNIST.shape) -->
<!-- ``` -->
<!-- $>>>$    (784, 2956) (1, 2956) -->
<!-- See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler -->
<!-- ```python -->
<!-- (X_train_MNIST.min(axis = 0)[:5], X_train_MNIST.max(axis = 0)[:5]),\ -->
<!-- (X_test_MNIST.min(axis = 0)[:5], X_test_MNIST.max(axis = 0)[:5]) -->
<!-- ``` -->
<!-- >    ((array([0.5, 0.5, 0.5, 0.5, 0.5]), array([0.7, 0.7, 0.7, 0.7, 0.7])), -->
<!--      (array([0.5, 0.5, 0.5, 0.5, 0.5]), array([0.7, 0.7, 0.7, 0.7, 0.7]))) -->
<!-- ```python -->
<!-- print( -->
<!--     "Train, 0:\t",len(np.squeeze(np.where(Y_train_MNIST == 0))[1,:])/Y_train_MNIST.shape[1],\ -->
<!--       "\nTrain, 1: \t",len(np.squeeze(np.where(Y_train_MNIST == 1))[1,:])/Y_train_MNIST.shape[1] -->
<!-- ) -->
<!-- print( -->
<!--     "Test, 0:\t",len(np.squeeze(np.where(Y_test_MNIST == 0))[1,:])/Y_test_MNIST.shape[1],\ -->
<!--       "\nTest, 1: \t",len(np.squeeze(np.where(Y_test_MNIST == 1))[1,:])/Y_test_MNIST.shape[1] -->
<!-- ) -->
<!-- ``` -->
<!-- >    Train, 0:	 0.46354871447902574  -->
<!--     Train, 1: 	 0.5364512855209743 -->
<!--     Test, 0:	 0.4810554803788904  -->
<!--     Test, 1: 	 0.5189445196211097 -->
<!-- ```python -->
<!-- where_0 = np.squeeze(np.where(Y_train_MNIST == 0))[1,:10] -->
<!-- where_1 = np.squeeze(np.where(Y_train_MNIST == 1))[1,:10] -->
<!-- plt.figure(figsize = (15,10)) -->
<!-- pick = np.asarray(where_0) -->
<!-- zero_images = np.array([], dtype = np.int64).reshape(28,0) -->
<!-- images = [np.reshape(X_train_MNIST[:,pick[image_index]], (28,28)) for image_index in range(10) ] -->
<!-- for image in images: -->
<!--     zero_images = np.concatenate([zero_images,image], axis = 1) -->
<!-- pick = np.asarray(where_1) -->
<!-- one_images = np.array([], dtype = np.int64).reshape(28,0) -->
<!-- images = [np.reshape(X_train_MNIST[:,pick[image_index]], (28,28)) for image_index in range(10) ] -->
<!-- for image in images: -->
<!--     one_images = np.concatenate([one_images,image], axis = 1) -->
<!-- both = np.concatenate([zero_images,one_images], axis = 0)     -->
<!-- plt.axis("off") -->
<!-- plt.imshow(both, cmap = "binary") -->
<!-- plt.show() -->
<!-- ``` -->
<div class="figure">
<img src="figures/output_63_0.png" alt="" />
<p class="caption">A subsample of digits “0” and “1” in the MNIST database.</p>
</div>
<p>One can also visualize the trainable weights as heatmaps (see full code in <a href="https://github.com/rafael-a-monteiro-math/Binary_classification_phase_separation/blob/master/Notebook_PSBC_examples.ipynb">Notebook_examples.ipynb</a>).</p>
<!-- ```python -->
<!-- parent_folder = "Examples/" -->
<!-- ## Non-subordinate ######################################################## -->
<!-- sub_non_sub = {} -->
<!-- folder_now = parent_folder + "W1S-NS/simulation1/" -->
<!-- with open(folder_now + "Full_model_properties.p", 'rb') as fp: Full_model_properties = pickle.load(fp) -->
<!-- sub_non_sub["non" + "best_par_P"] = Full_model_properties["best_par_P_model"] -->
<!-- sub_non_sub["non" + "best_par_U"] = Full_model_properties["best_par_U_model"] -->
<!-- ## Subordinate  ############################################################ -->
<!-- parent_folder = "Examples/" -->
<!-- folder_now = parent_folder + "W1S-S/simulation1/" -->
<!-- with open(folder_now + "Full_model_properties.p", 'rb') as fp: Full_model_properties = pickle.load(fp) -->
<!-- sub_non_sub["sub" + "best_par_P"] = Full_model_properties["best_par_P_model"] -->
<!-- sub_non_sub["sub" + "best_par_U"] = Full_model_properties["best_par_U_model"] -->
<!-- ## W1S-Nt8 ################################################################## -->
<!-- parent_folder = "Examples/" -->
<!-- folder_now = parent_folder + "W1S-Nt8/simulation1/" -->
<!-- with open(folder_now + "Full_model_properties.p", 'rb') as fp: Full_model_properties = pickle.load(fp) -->
<!-- sub_non_sub["kfold" + "best_par_P"] = Full_model_properties["best_par_P_model"] -->
<!-- sub_non_sub["kfold" + "best_par_U"] = Full_model_properties["best_par_U_model"] -->
<!-- ``` -->
<!-- ```python -->
<!-- import seaborn as sns -->
<!-- %matplotlib inline -->
<!-- ``` -->
<!-- ```python -->
<!-- f, ax = plt.subplots(ncols = 3, nrows = 2, figsize = (15,10), constrained_layout = False) -->
<!-- list_sub  = ("non", "sub", "kfold") -->
<!-- list_sub_big  = ("Non-subordinate\n(Weights-1-sharing)",\ -->
<!--                  "Subordinate\n(Weights-1-sharing)",\ -->
<!--                  "Subordinate,"+ r"$\mathrm{N_t}=8$"+"\n"+r"(Weights-$1$-sharing)") -->
<!-- m_1 = np.min([sub_non_sub[list_sub[i] + "best_par_P"]["alpha_x_t"].min() for i in range(3)]) -->
<!-- M_1 = np.max([sub_non_sub[list_sub[i] + "best_par_P"]["alpha_x_t"].max() for i in range(3)]) -->
<!-- for i in range(3): -->
<!--     Nx = sub_non_sub[list_sub[i] + "best_par_P"]["alpha_x_t"].shape[0] -->
<!--     sub_non_sub[str(i) + "matrix"] = sub_non_sub[list_sub[i] + "best_par_P"]["alpha_x_t"] -->
<!--     sns.heatmap(\ -->
<!--                 sub_non_sub[str(i) + "matrix"],  ax = ax[0,i], vmin = m_1, vmax = M_1, cbar = False,\ -->
<!--                 cmap = 'inferno' -->
<!--                ) -->
<!--     pcm = ax[0,i].pcolormesh(sub_non_sub[str(i) + "matrix"]) -->
<!--     ax[0,i].set_title(list_sub_big[i], size = 20) -->
<!-- f.subplots_adjust(right=0.9) -->
<!-- cbar_ax = f.add_axes([.92, .55, .03, .3]) -->
<!-- f.colorbar(pcm, cax = cbar_ax) -->
<!-- ax[0,0].set_ylabel(r"$W_P^{[\cdot]} = \beta^{[\cdot]}$",rotation=90, size = 18) -->
<!-- m_2 = np.min([sub_non_sub[list_sub[i] + "best_par_U"]["alpha_x_t"].min() for i in range(3)]) -->
<!-- M_2 = np.max([sub_non_sub[list_sub[i] + "best_par_U"]["alpha_x_t"].max() for i in range(3)]) -->
<!-- for i in range(3): -->
<!--     Nx = sub_non_sub[list_sub[i] + "best_par_U"]["alpha_x_t"].shape[0] -->
<!--     M = sub_non_sub[list_sub[i] + "best_par_U"]["alpha_x_t"] -->
<!--     sub_non_sub[str(i) + "matrix"] = M -->
<!--     sns.heatmap(M, ax = ax[1,i], vmin = m_2, vmax = M_2, cbar = False, cmap = 'inferno') -->
<!--     pcM = ax[1,i].pcolormesh(sub_non_sub[str(i) + "matrix"]) -->
<!-- cbar_ax2 = f.add_axes([.92, .15, .03, .3]) -->
<!-- f.colorbar(pcM, cax = cbar_ax2) -->
<!-- ax[1,0].set_ylabel(r"$W_U^{[\cdot]} = \alpha^{[\cdot]}$", rotation = 90, size = 18) -->
<!-- plt.show() -->
<!-- ``` -->
<div class="figure">
<img src="figures/output_67_0.png" alt="" />
<p class="caption">Heatmaps of trainable weights for some trained PSBC models in the Examples folder.</p>
</div>
<div id="retrieving-some-statistics" class="section level2">
<h2><span class="header-section-number">4.1</span> Retrieving some statistics</h2>
<p>To get a flavor of what is in the statistics folder, we first need to retrieve some of this data:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="sec-mnist.html#cb3-1"></a>parameters_MNIST_nondif, stats_folder_MNIST <span class="op">=</span> {}, <span class="st">&quot;Statistics/MNIST/&quot;</span></span>
<span id="cb3-2"><a href="sec-mnist.html#cb3-2"></a><span class="cf">with</span> <span class="bu">open</span>(stats_folder_MNIST <span class="op">+</span> <span class="st">&quot;parameters_MNIST_nondif.p&quot;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> fp: </span>
<span id="cb3-3"><a href="sec-mnist.html#cb3-3"></a>    parameters_MNIST_nondif <span class="op">=</span> pickle.load(fp)</span>
<span id="cb3-4"><a href="sec-mnist.html#cb3-4"></a></span>
<span id="cb3-5"><a href="sec-mnist.html#cb3-5"></a>parameters_MNIST_Neumann, stats_folder_MNIST <span class="op">=</span> {}, <span class="st">&quot;Statistics/MNIST/&quot;</span></span>
<span id="cb3-6"><a href="sec-mnist.html#cb3-6"></a><span class="cf">with</span> <span class="bu">open</span>(stats_folder_MNIST <span class="op">+</span> <span class="st">&quot;parameters_MNIST_Neumann.p&quot;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> fp:</span>
<span id="cb3-7"><a href="sec-mnist.html#cb3-7"></a>        parameters_MNIST_Neumann <span class="op">=</span> pickle.load(fp)</span>
<span id="cb3-8"><a href="sec-mnist.html#cb3-8"></a></span>
<span id="cb3-9"><a href="sec-mnist.html#cb3-9"></a>parameters_MNIST_Periodic, stats_folder_MNIST <span class="op">=</span> {}, <span class="st">&quot;Statistics/MNIST/&quot;</span></span>
<span id="cb3-10"><a href="sec-mnist.html#cb3-10"></a><span class="cf">with</span> <span class="bu">open</span>(stats_folder_MNIST <span class="op">+</span> <span class="st">&quot;parameters_MNIST_Periodic.p&quot;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> fp:</span>
<span id="cb3-11"><a href="sec-mnist.html#cb3-11"></a>        parameters_MNIST_Periodic <span class="op">=</span> pickle.load(fp)</span></code></pre></div>
<p>The function <em>accuracies</em> is part of the module <em>aux_fnts_for_jupyter_notebooks.py</em>, which is available in this Github <span class="citation">(Monteiro <a href="#ref-Bin_phase_github" role="doc-biblioref">2020</a><a href="#ref-Bin_phase_github" role="doc-biblioref">b</a>)</span>. As before, help on this equation can be called typing ‘help(accuracies)’.</p>
<!-- Help on function accuracies in module aux_fnts_for_jupyter_notebooks: -->
<!-- accuracies(parameters, name, accuracy_type, number_folders=10, number_simulations=10) -->
<!--     This function is only used in the jupyter notebook for the MNIST dataset -->
<!--     Parameters -->
<!--     ---------- -->
<!--     parameters : dictionary -->
<!--         Dictionary containing summary of data for some PSBC experiments. -->
<!--     name : string -->
<!--         Name of the keys of the dictionary "parameters" that we are studying, corresponding to a PSBC configuration. -->
<!--     accuracy_type : string -->
<!--         Either "best_accuracy_train" or "best_accuracy_test". -->
<!--     number_folders : {int, 10}, optional -->
<!--         Number of folders, where each folder corresponds of one value of the parameter being valued. -->
<!--     number_simulations : {int, 10}, optional -->
<!--         Number of simulations that were run with the same parameter, for statistical purposes. -->
<!--     Returns -->
<!--     ------- -->
<!--     A : matrix -->
<!--         Matrix with all the accuracies of type accuracy_type,  -->
<!--         where each row has all the simulations for a certain parameter -->
<!--     value_of_parameter_varying : list  -->
<!--         list with parameters values. -->
<!-- For more details about the code in the next plot, see Remark 1 (cell 20) in the jupyter-notebook [Notebook_examples.ipynb](https://github.com/rafael-a-monteiro-math/Binary_classification_phase_separation/blob/master/Notebook_PSBC_examples.ipynb). -->
<!-- ```python -->
<!-- A_train_1NS, value_of_parameter_varying = accuracies ( -->
<!--     parameters_MNIST_nondif, "W1S-NS", "best_accuracy_train") -->
<!-- value_of_parameter_varying = value_of_parameter_varying[::-1] -->
<!-- average_train_1NS, stdev_train_1NS =\ -->
<!-- np.mean(A_train_1NS, axis = 1)[::-1], np.std(A_train_1NS, axis = 1)[::-1] -->
<!-- A_train_1S, _ = accuracies ( -->
<!--     parameters_MNIST_nondif, "W1S-S", "best_accuracy_train") -->
<!-- average_train_1S, stdev_train_1S = np.mean(A_train_1S, axis = 1)[::-1], np.std(A_train_1S, axis = 1)[::-1] -->
<!-- A_train_NS, _ = accuracies ( -->
<!--     parameters_MNIST_nondif, "WNtS-NS", "best_accuracy_train") -->
<!-- average_train_NS, stdev_train_NS = np.mean(A_train_NS, axis = 1)[::-1], np.std(A_train_NS, axis = 1)[::-1] -->
<!-- A_train_S, _ = accuracies ( -->
<!--     parameters_MNIST_nondif, "WNtS-S", "best_accuracy_train") -->
<!-- average_train_S, stdev_train_S = np.mean(A_train_S, axis = 1)[::-1], np.std(A_train_S, axis = 1)[::-1] -->
<!-- colors = pl.cm.tab10(np.linspace(0,1,9)) -->
<!-- colors_markers = pl.cm.tab10(np.linspace(0,1,9)) -->
<!-- # Parameters using during pltting -->
<!-- col_NS, mk_NS, alpha_for_all, L, T  = 0, 1, 0.6, -3, .008 -->
<!-- value_of_parameter_varying = np.asarray(value_of_parameter_varying) -->
<!-- fig,ax = plt.subplots( -->
<!--      nrows =  2, ncols = 2, figsize = (15,12), sharey='row', sharex = 'col',\ -->
<!--    gridspec_kw = {'wspace':0,'hspace':0},    constrained_layout = False -->
<!-- ) -->
<!-- markers, caps, bars = ax[0,0].errorbar( -->
<!--     (1 + T)*value_of_parameter_varying[:L], average_train_1NS[:L], marker = 'o',\ -->
<!--     yerr = stdev_train_1NS[:L], errorevery = 1, linestyle = '-',\ -->
<!--     label = "Non Sub", color = colors[col_NS+3],\ -->
<!--     fillstyle = 'none', markeredgecolor = colors_markers[mk_NS-1] -->
<!-- ) -->
<!-- [bar.set_alpha(alpha_for_all) for bar in bars] -->
<!-- markers, caps, bars = ax[0,0].errorbar( -->
<!--     (1/(1 + T))*value_of_parameter_varying[:L], average_train_1S[:L], marker = 'x',\ -->
<!--     yerr = stdev_train_1S[:L], errorevery = 1, linestyle = '-',\ -->
<!--     label = "Sub", color = colors[col_NS],\ -->
<!--     fillstyle = 'none', markeredgecolor = colors_markers[mk_NS+2]     -->
<!-- ) -->
<!-- [bar.set_alpha(alpha_for_all) for bar in bars] -->
<!-- markers, caps, bars = ax[0,1].errorbar( -->
<!--     (1 + T)*value_of_parameter_varying[:L], average_train_NS[:L], marker = 'o',\ -->
<!--     yerr = stdev_train_NS[:L], errorevery = 1, linestyle = '-',\ -->
<!--     label = "Non Sub", color = colors[col_NS+3],\ -->
<!--     fillstyle = 'none', markeredgecolor = colors_markers[mk_NS-1] -->
<!-- ) -->
<!-- [bar.set_alpha(alpha_for_all) for bar in bars] -->
<!-- markers, caps, bars = ax[0,1].errorbar( -->
<!--     (1/(1 + T))*value_of_parameter_varying[:L], average_train_S[:L], marker = 'x',\ -->
<!--     yerr = stdev_train_S[:L], errorevery = 1, linestyle = '-',\ -->
<!--     label = "Sub", color = colors[col_NS],\ -->
<!--     fillstyle = 'none', markeredgecolor = colors_markers[mk_NS+2]     -->
<!-- ) -->
<!-- [bar.set_alpha(alpha_for_all) for bar in bars] -->
<!-- A_test_1NS, _ = accuracies (parameters_MNIST_nondif, "W1S-NS", "best_accuracy_test") -->
<!-- average_test_1NS, stdev_test_1NS = np.mean(A_test_1NS, axis = 1)[::-1], np.std(A_test_1NS, axis = 1)[::-1] -->
<!-- A_test_1S, _ = accuracies (parameters_MNIST_nondif, "W1S-S", "best_accuracy_test") -->
<!-- average_test_1S, stdev_test_1S = np.mean(A_test_1S, axis = 1)[::-1], np.std(A_test_1S, axis = 1)[::-1] -->
<!-- A_test_NS, _ = accuracies (parameters_MNIST_nondif, "WNtS-NS", "best_accuracy_test") -->
<!-- average_test_NS, stdev_test_NS = np.mean(A_test_NS, axis = 1)[::-1], np.std(A_test_NS, axis = 1)[::-1] -->
<!-- A_test_S, _ = accuracies (parameters_MNIST_nondif, "WNtS-S", "best_accuracy_test") -->
<!-- average_test_S, stdev_test_S = np.mean(A_test_S, axis = 1)[::-1], np.std(A_test_S, axis = 1)[::-1] -->
<!-- markers, caps, bars = ax[1,0].errorbar( -->
<!--     (1 + T)*value_of_parameter_varying[:L], average_test_1NS[:L], marker = 'o', \ -->
<!--     yerr = stdev_test_1NS[:L], errorevery = 1, linestyle = '-',\ -->
<!--     label = "Non Sub", color = colors[col_NS+3],\ -->
<!--     fillstyle = 'none', markeredgecolor = colors_markers[mk_NS-1] -->
<!-- ) -->
<!-- [bar.set_alpha(alpha_for_all) for bar in bars] -->
<!-- markers, caps, bars = ax[1,0].errorbar( -->
<!--     (1/(1 + T))*value_of_parameter_varying[:L], average_test_1S[:L], marker = 'x',\ -->
<!--     yerr = stdev_test_1S[:L], errorevery = 1,  linestyle = '-',\ -->
<!--     label = "Sub", color = colors[col_NS],\ -->
<!--     fillstyle = 'none', markeredgecolor = colors_markers[mk_NS+2] -->
<!-- ) -->
<!-- [bar.set_alpha(alpha_for_all) for bar in bars] -->
<!-- markers, caps, bars = ax[1,1].errorbar( -->
<!--     (1 + T)*value_of_parameter_varying[:L], average_test_NS[:L], marker = 'o', \ -->
<!--     yerr = stdev_test_NS[:L], errorevery = 1, linestyle = '-',\ -->
<!--     label = "Non-sub", color = colors[col_NS+3],\ -->
<!--     fillstyle = 'none', markeredgecolor = colors_markers[mk_NS-1] -->
<!-- ) -->
<!-- [bar.set_alpha(alpha_for_all) for bar in bars] -->
<!-- markers, caps, bars = ax[1,1].errorbar( -->
<!--     (1/(1 + T))*value_of_parameter_varying[:L], average_test_S[:L], marker = 'x',\ -->
<!--     yerr = stdev_test_S[:L], errorevery = 1,  linestyle = '-',\ -->
<!--     label = "Sub", color = colors[col_NS],\ -->
<!--     fillstyle = 'none', markeredgecolor = colors_markers[mk_NS+2] -->
<!-- ) -->
<!-- [bar.set_alpha(alpha_for_all) for bar in bars] -->
<!-- for i in [0,1]: -->
<!--     for j in [0,1]: -->
<!--         ax[i,j].grid(axis = "y") -->
<!--         ax[i,j].set_xscale('log')  -->
<!--         ax[i,j].set_xticks(np.asarray(value_of_parameter_varying[:L])) -->
<!--         ax[i,j].tick_params("x") -->
<!--         ax[i,j].get_xaxis().set_major_formatter(ScalarFormatter()) -->
<!--         ax[i,j].xaxis.set_minor_formatter(plt.matplotlib.ticker.NullFormatter()) -->
<!-- ax[0,0].set_title(r"Weights-$1$-sharing", size = 18)   -->
<!-- ax[0,1].set_title(r"Weights-$\mathrm{N_t}$-sharing", size = 18)   -->
<!-- ax[0,0].set_ylabel("Accuracy train (average)")    -->
<!-- ax[1,0].set_ylabel("Accuracy test (average)") -->
<!-- ax[1,0].set_xlabel(r"$\mathrm{N_{ptt}}$ (in logarithmic scale)") -->
<!-- ax[1,1].set_xlabel(r"$\mathrm{N_{ptt}}$ (in logarithmic scale)") -->
<!-- chartBox = ax[1,1].get_position() -->
<!-- ax[1,1].set_position([chartBox.x0, chartBox.y0, chartBox.width*0.2, chartBox.height]) -->
<!-- ax[1,1].legend(loc = 'upper center', bbox_to_anchor = (0, 0.15), shadow = True, ncol = 4) -->
<!-- fig.set_tight_layout({'rect': [0, 0, 1, 0.95]}) -->
<!-- fig.suptitle("Non-diffusive PSBC", size = 20) -->
<!-- plt.show() -->
<!-- ``` -->
<p>With data in the Statistics folder we can plot the graph of accuracies of the non-diffusive PSBC for different values of partition cardinality <span class="math inline">\(\mathrm{N_{ptt}}\)</span>.</p>
<div class="figure">
<img src="figures/output_72_0.png" style="width:90.0%" alt="" />
<p class="caption">A comparison of average accuracy of the non-diffusive PSBC for different values of Partition cardinality; models compared either have subordinate phase (tagged as “Sub”) or not (non-subordinate phase, tagged as “Non-sub”).</p>
</div>
<p>If can also see the evolution of the maximum of trainable weights over epochs, for a Periodic PSBC with <span class="math inline">\(\mathrm{N_t} =1\)</span>.</p>
<!-- ```python -->
<!-- fig, ax = plt.subplots(nrows = 2, figsize = (15,10)) -->
<!-- _, value_of_parameter_varying = accuracies ( -->
<!--     parameters_MNIST_Periodic, "Per_W1S-Nt2", "best_accuracy_train", number_folders = 13) -->
<!-- colors = pl.cm.tab20(np.linspace(0,1,16)) -->
<!-- def test_label_value_of_parameter_varying(x): -->
<!--     if x == 0: -->
<!--         return "0" -->
<!--     j = int(np.ceil(np.log2(x)))  -->
<!--     return r'$2^{{{0}}}$'.format(j) -->
<!-- param = parameters_MNIST_Periodic["Per_W1S-Nt8"] -->
<!-- for i in range(1, 14): -->
<!--     param_now = param[str(i)] -->
<!--     diam_hist_now = param_now['diam_hist'] -->
<!--     ax[0].plot(diam_hist_now["U"],   linestyle = '-', lw = 3,\ -->
<!--                label = str(test_label_value_of_parameter_varying(value_of_parameter_varying[i-1])),\ -->
<!--                color=colors[i] ) -->
<!--     ax[1].plot(diam_hist_now["P"], linestyle = (0,(3,1,1,1,1,1)), lw = 3,\ -->
<!--                label = str(test_label_value_of_parameter_varying(value_of_parameter_varying[i-1])),\ -->
<!--                color = colors[i]) -->
<!-- ax[0].legend(loc = 2, fontsize = 16, ncol = 3) -->
<!-- ax[0].set_ylabel(r'Diameter $\alpha^{[\cdot]}$') -->
<!-- ax[0].set_xlabel('Number of iterations') -->
<!-- ax[0].grid(True) -->
<!-- ax[0].legend(loc = 2,  ncol = 3, title = r"$\varepsilon$") -->
<!-- plt.rcParams["legend.title_fontsize"] = 20 -->
<!-- plt.rcParams["legend.columnspacing"] = .8 -->
<!-- ax[1].set_ylabel(r'Diameter $\beta^{[\cdot]}$') -->
<!-- ax[1].set_xlabel('Number of iterations') -->
<!-- ax[1].grid(True) -->
<!-- plt.show() -->
<!-- ``` -->
<div class="figure">
<img src="figures/output_75_1.png" style="width:80.0%" alt="" />
<p class="caption">The behavior of <span class="math inline">\(\mathscr{P}_{\alpha}^{[\cdot]}\)</span> and <span class="math inline">\(\mathscr{P}_{\beta}^{[\cdot]}\)</span> over time, for a Periodic PSBC with <span class="math inline">\(\mathrm{N_t} =1\)</span>.</p>
</div>
<p>With these data we can also plot confusion matrices.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="sec-mnist.html#cb4-1"></a>parent_folder <span class="op">=</span> <span class="st">&quot;Examples/&quot;</span></span>
<span id="cb4-2"><a href="sec-mnist.html#cb4-2"></a>folder_now <span class="op">=</span> parent_folder <span class="op">+</span> <span class="st">&quot;W1S-Nt8/simulation1/&quot;</span></span>
<span id="cb4-3"><a href="sec-mnist.html#cb4-3"></a><span class="cf">with</span> <span class="bu">open</span>(folder_now <span class="op">+</span> <span class="st">&quot;Full_model_properties.p&quot;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> fp: Full_model_properties <span class="op">=</span> pickle.load(fp)</span></code></pre></div>
<!-- ```python -->
<!-- best_predic_vector_test_now = np.squeeze(Full_model_properties["best_predic_vector_test"]) -->
<!-- real_tags = np.squeeze(Y_test_MNIST) -->
<!-- ``` -->
<!-- ```python -->
<!-- from sklearn.metrics import confusion_matrix -->
<!-- ``` -->
<!-- ```python -->
<!-- conf_matrix_example = confusion_matrix(real_tags, best_predic_vector_test_now ) -->
<!-- ``` -->
<!-- ```python -->
<!-- fig, axs = plt.subplots( nrows = 1,ncols = 1, figsize = (10, 10)) -->
<!-- cax_test =   axs.matshow(conf_matrix_example, cmap = plt.cm.Blues) -->
<!-- axs = sns.heatmap(conf_matrix_example/np.sum(conf_matrix_example),\ -->
<!--                   annot = True, annot_kws={"size": 20},\ -->
<!--                   fmt='.2%', cmap = 'Blues', ax = axs, cbar = False) -->
<!-- plt.yticks(rotation = 0, fontsize = 18) -->
<!-- plt.xticks(fontsize = 18) -->
<!-- axs.xaxis.tick_top() # x axis on top -->
<!-- axs.xaxis.set_label_position('top') -->
<!-- axs.set_ylabel("True labels", fontsize = 22) -->
<!-- axs.set_xlabel("Predicted labels", fontsize = 22) -->
<!-- plt.show() -->
<!-- ``` -->
<div class="figure">
<img src="figures/output_80_0.png" style="width:50.0%" alt="" />
<p class="caption">Confusion matrix of a realization of the diffusive PSBC with Neumann BCs, weights-1-sharing, and <span class="math inline">\(\mathrm{N_t = 8}\)</span>.</p>
</div>
<p>There are other things that we do as well: for instance, we can plot Table 4 in the <a href="https://github.com/rafael-a-monteiro-math/Binary_classification_phase_separation/blob/master/Supplement.pdf">Supplement</a>, which shows the average value of the maximum (in <span class="math inline">\(\ell^{\infty}\)</span>-norm) of trainable weights of non-diffusive PSBC models for different values of partition cardinality <span class="math inline">\(\mathrm{N_t}\)</span>.</p>
<div class="figure">
<img src="figures/max_non-dif.png" style="width:80.0%" alt="" />
<p class="caption">The maximum over epochs. You find the code for this plot in <a href="https://github.com/rafael-a-monteiro-math/Binary_classification_phase_separation/blob/master/Notebook_PSBC_examples.ipynb">Notebook_examples.ipynb</a>.</p>
</div>
</div>
<div id="a-homemade-example-hadwritten-0-and-1" class="section level2">
<h2><span class="header-section-number">4.2</span> A “homemade” example: hadwritten 0 and 1</h2>
<p>We now step foward to a higher dimensional feature space, using equations that are similar to those shown in Section <a href="sec-1d.html#sec:1d">3</a> of this tutorial. There is an interesting interplay between high-dimensionality of feature spaces and model compressibility, which we highlight here by applying the PSBC to the subset “0”-“1” of the MNIST database; we once more refer the reader to Sections 4 and 5 in the <a href="https://arxiv.org/abs/2009.02467">paper</a> for more details.</p>
<p>The goal in this section is to illustrate a bit more of the PSBC’s use by predicting the label for some of the author’s own handwritten numbers. For that we shall use the trainable models available in <span class="citation">(Monteiro <a href="#ref-Bin_phase_data" role="doc-biblioref">2020</a><a href="#ref-Bin_phase_data" role="doc-biblioref">a</a>)</span>, in the tarball <strong>PSBC_Examples.tar.gz</strong>. Two of these digits are shown below.</p>
<div class="figure">
<img src="figures/output_85_0.png" alt="" />
<p class="caption">Two of the author’s own handwritten numbers; original photo, treated using Gimp.</p>
</div>
<p>In fact, we shall use 6 of the author’s handwritten digits - 3 zeros, 3 ones - for this notebook. If you read the first papers of LeCun et al. about the MNIST project, there is a description of the way pictures were taken (see for instance, Section III A in <span class="citation">(Lecun et al. <a href="#ref-Mnist" role="doc-biblioref">1998</a>)</span>, or the explanation in <a href="http://yann.lecun.com/exdb/mnist/">LeCun’s MNIST webpage</a>), so that they look the way they do in cell 36 of <a href="https://github.com/rafael-a-monteiro-math/Binary_classification_phase_separation/blob/master/Notebook_PSBC_examples.ipynb">Notebook_examples.ipynb</a>: the images had to be controlled for angle, centralization, etc; this is part of the statistical design, which I tried to follow here without too much concern (because this is just a tutorial) but, somehow, “as close as possible”.</p>
<p>Pictures we cropped using <a href="https://www.gimp.org">GIMP</a>, a free software for image manipulation: you take a picture, crop it, go to image, set it into grayscale, adjust for light contrast and other things. And that’s it.</p>
<p>Now, with them cropped, “MNIST-like” grayscale images in hands, you proceed as in the next cell, reshaping these pictures as a 28 x 28 matrix. We show what the two digits that you saw before will look like.</p>
<!-- ```python -->
<!-- from PIL import Image -->
<!-- ``` -->
<!-- ```python -->
<!-- im_array0 = np.asarray(Image.open("figures/my_0.jpg")) -->
<!-- im_array1 = np.asarray(Image.open("figures/my_1.jpg")) -->
<!-- fig, ax = plt.subplots(1,2) -->
<!-- ax[0].imshow(im_array0, cmap='binary') -->
<!-- ax[1].imshow(im_array1, cmap='binary') -->
<!-- ax[0].axis(False) -->
<!-- ax[1].axis(False) -->
<!-- plt.show() -->
<!-- ``` -->
<!-- ![Two of my own handwritten numbers; original photo, treated using Gimp.](output_85_0.png) -->
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="sec-mnist.html#cb5-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb5-2"><a href="sec-mnist.html#cb5-2"></a></span>
<span id="cb5-3"><a href="sec-mnist.html#cb5-3"></a><span class="kw">def</span> create_MNIST_type_figure(name):</span>
<span id="cb5-4"><a href="sec-mnist.html#cb5-4"></a>    <span class="co">&quot;&quot;&quot;Convert jpg figure to a (28,28) numpy array&quot;&quot;&quot;</span></span>
<span id="cb5-5"><a href="sec-mnist.html#cb5-5"></a>    image <span class="op">=</span> Image.<span class="bu">open</span>(name).convert(<span class="st">&#39;L&#39;</span>)</span>
<span id="cb5-6"><a href="sec-mnist.html#cb5-6"></a>    image2 <span class="op">=</span> image.resize((<span class="dv">28</span>,<span class="dv">28</span>))</span>
<span id="cb5-7"><a href="sec-mnist.html#cb5-7"></a>    im2_as_array <span class="op">=</span> <span class="dv">255</span><span class="op">-</span> np.array(image2,  dtype<span class="op">=</span>np.uint8)</span>
<span id="cb5-8"><a href="sec-mnist.html#cb5-8"></a>    <span class="bu">print</span>(<span class="st">&quot;image has shape&quot;</span>, im2_as_array.shape)</span>
<span id="cb5-9"><a href="sec-mnist.html#cb5-9"></a></span>
<span id="cb5-10"><a href="sec-mnist.html#cb5-10"></a>    <span class="cf">return</span> im2_as_array</span>
<span id="cb5-11"><a href="sec-mnist.html#cb5-11"></a></span>
<span id="cb5-12"><a href="sec-mnist.html#cb5-12"></a>my_0 <span class="op">=</span> create_MNIST_type_figure(<span class="st">&quot;figures/my_0.jpg&quot;</span>)</span>
<span id="cb5-13"><a href="sec-mnist.html#cb5-13"></a>my_0_v2 <span class="op">=</span> create_MNIST_type_figure(<span class="st">&quot;figures/my_0_v2.jpg&quot;</span>)</span>
<span id="cb5-14"><a href="sec-mnist.html#cb5-14"></a>my_0_v3 <span class="op">=</span> create_MNIST_type_figure(<span class="st">&quot;figures/my_0_v3.jpg&quot;</span>)</span>
<span id="cb5-15"><a href="sec-mnist.html#cb5-15"></a>my_1 <span class="op">=</span> create_MNIST_type_figure(<span class="st">&quot;figures/my_1.jpg&quot;</span>)</span>
<span id="cb5-16"><a href="sec-mnist.html#cb5-16"></a>my_1_v2 <span class="op">=</span> create_MNIST_type_figure(<span class="st">&quot;figures/my_1_v2.jpg&quot;</span>)</span>
<span id="cb5-17"><a href="sec-mnist.html#cb5-17"></a>my_1_v3 <span class="op">=</span> create_MNIST_type_figure(<span class="st">&quot;figures/my_1_v3.jpg&quot;</span>)</span>
<span id="cb5-18"><a href="sec-mnist.html#cb5-18"></a></span>
<span id="cb5-19"><a href="sec-mnist.html#cb5-19"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>,<span class="dv">2</span>)</span>
<span id="cb5-20"><a href="sec-mnist.html#cb5-20"></a>ax[<span class="dv">0</span>].imshow(my_0, cmap<span class="op">=</span><span class="st">&#39;binary&#39;</span>)</span>
<span id="cb5-21"><a href="sec-mnist.html#cb5-21"></a>ax[<span class="dv">1</span>].imshow(my_1, cmap<span class="op">=</span><span class="st">&#39;binary&#39;</span>)</span>
<span id="cb5-22"><a href="sec-mnist.html#cb5-22"></a>ax[<span class="dv">0</span>].axis(<span class="va">False</span>)</span>
<span id="cb5-23"><a href="sec-mnist.html#cb5-23"></a>ax[<span class="dv">1</span>].axis(<span class="va">False</span>)</span>
<span id="cb5-24"><a href="sec-mnist.html#cb5-24"></a>plt.show()</span></code></pre></div>
<p>You will get this
<!--     image has shape (28, 28) -->
<!--     image has shape (28, 28) -->
<!--     image has shape (28, 28) -->
<!--     image has shape (28, 28) -->
<!--     image has shape (28, 28) -->
<!--     image has shape (28, 28) --></p>
<div class="figure">
<img src="figures/output_87_1.png" alt="" />
<p class="caption">The handwritten digits shown above, now as 28X28 pixels images.</p>
</div>
<p>Recall that we need to flatten these matrices,</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="sec-mnist.html#cb6-1"></a>my_0_for_psbc <span class="op">=</span> my_0.flatten(order<span class="op">=</span><span class="st">&#39;C&#39;</span>)</span>
<span id="cb6-2"><a href="sec-mnist.html#cb6-2"></a>...</span></code></pre></div>
<p>and we can then combine all these 6 flattened images as columns in a single matrix <strong>combined_handwritten</strong>, whose size is <span class="math inline">\(784\times 6\)</span>.</p>
<p>Now we initialize the PSBC model</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="sec-mnist.html#cb7-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;Examples/W1S-Nt8/simulation1/Full_model_properties.p&quot;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> fp: load_mnist <span class="op">=</span> pickle.load(fp)</span>
<span id="cb7-2"><a href="sec-mnist.html#cb7-2"></a></span>
<span id="cb7-3"><a href="sec-mnist.html#cb7-3"></a>psbc_testing <span class="op">=</span> Binary_Phase_Separation()</span></code></pre></div>
<p>and apply to our matrix/data <strong>combined_handwritten</strong> with the trained weights we have chosen.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="sec-mnist.html#cb8-1"></a>prediction <span class="op">=</span> psbc_testing.predict(combined_handwritten, load_mnist[<span class="st">&quot;best_par_U_model&quot;</span>],load_mnist[<span class="st">&quot;best_par_P_model&quot;</span>])</span>
<span id="cb8-2"><a href="sec-mnist.html#cb8-2"></a><span class="bu">print</span>(prediction)</span></code></pre></div>
<p>…and we fail, as we see in the output below. Floating number overflows, “NAN”, etc.. sad news..</p>
<p><span class="math inline">\(&gt;&gt;&gt;\)</span></p>
<pre><code>[0 0 0 0 0 0]

/Users/rafaelmonteiro/Desktop/PSBC/All_cases/binary_phase_separation.py:475: RuntimeWarning: overflow encountered in multiply
  v = v +  dt * v * (1-v) * (v-alpha_x_t)
/Users/rafaelmonteiro/Desktop/PSBC/All_cases/binary_phase_separation.py:477: RuntimeWarning: invalid value encountered in matmul
  v = np.matmul(Minv,v)
/Users/rafaelmonteiro/Desktop/PSBC/All_cases/binary_phase_separation.py:1128: RuntimeWarning: invalid value encountered in greater
  keepdims = True, axis = 0))) &gt; .5, dtype = np.int32)</code></pre>
<p>This seems really bad… but do not despair: recall that data need to satisfy the <strong>normalization conditions</strong>: all the features have to be in the range <span class="math inline">\([0,1]\)</span>. We are in fact very far from that now: if you look for the minimum and maximum value of these matrices you will get <span class="math inline">\(0\)</span> and <span class="math inline">\(255\)</span>, respectively, as an answer.</p>
<p>With that said, let’s normalize the data:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="sec-mnist.html#cb10-1"></a>init_data <span class="op">=</span> Initialize_Data()</span></code></pre></div>
<!-- ```python -->
<!-- help(init_data) -->
<!-- ``` -->
<!-- $>>>$ -->
<!--     Help on Initialize_Data in module binary_phase_separation object: -->
<!--     class Initialize_Data(builtins.object) -->
<!--      |  This class preprocess the data, normalizing it. -->
<!--      |   -->
<!--      |  Methods defined here: -->
<!--      |   -->
<!--      |  __init__(self) -->
<!--      |      Class initializer. No returned value. -->
<!--      ... -->
<!-- |   -->
<!-- |  denormalize(self, Z, min_vals, max_vals, sigma=0.2) -->
<!-- |      'denormalize' method. -->
<!-- |       -->
<!-- |      This method puts the data back to its original scale. -->
<!-- |      Of the non-normalized data the method uses its  minimum value -->
<!-- |      min_vals, its original maxum value max_vals, and sigma. -->
<!-- |      The non-normalized data is transformed by -->
<!-- |       -->
<!-- |      A  = ( 1 / sigma ) * ( Z - .5 + sigma /2) -->
<!-- |       -->
<!-- |      and then Z_2 = min_vals + A * (max_vals - min_vals). -->
<!-- |       -->
<!-- |      Z_2 is the returned value. -->
<!-- |       -->
<!-- |      Parameters -->
<!-- |      ---------- -->
<!-- |       -->
<!-- |      Returns -->
<!-- |      ------- -->
<!-- |      Non-normalized data 'A'. -->
<!-- |       -->
<!-- |      A : numpy.ndarray -->
<!-- |   -->
<!-- |  normalize(self, Z, sigma=0.2) -->
<!-- |      'normalize' method. -->
<!-- |       -->
<!-- |      This method normalizes the data. The range of each  -->
<!-- |      coordinate of the normalized data takes values  -->
<!-- |      in between [0.5 - sigma/2, 0.5 + sigma/2] -->
<!-- |      Values cetralized at .5,  -->
<!-- |       -->
<!-- |      Parameters -->
<!-- |      ---------- -->
<!-- |       -->
<!-- |      Returns -->
<!-- |      ------- -->
<!-- |      Normalized data as 'Z_normalized',  -->
<!-- |      minimum value of non-normalized data as 'min_vals', -->
<!-- |      maximum value of non-normalized data as 'max_vals'. -->
<!-- |       -->
<!-- |      Z_normalized : numpy.ndarray, -->
<!-- |      min_vals : float, -->
<!-- |      max_vals : float. -->
<!-- |   -->
<!-- |  ---------------------------------------------------------------------- -->
<!-- |  Data descriptors defined here: -->
<!-- |   -->
<!-- |  __dict__ -->
<!-- |      dictionary for instance variables (if defined) -->
<!-- |   -->
<!-- |  __weakref__ -->
<!-- |      list of weak references to the object (if defined) -->
<p>So, the data get’s normalized, but centered. By default, it gets rescaled in the range [0.4,0.6]. What we do then is: (i) we normalize it, then (ii) we add 0.1 to it.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="sec-mnist.html#cb11-1"></a>combined_handwritten_for_psbc, _, _ <span class="op">=</span> init_data.normalize(combined_handwritten)</span>
<span id="cb11-2"><a href="sec-mnist.html#cb11-2"></a>combined_handwritten_for_psbc <span class="op">=</span> <span class="fl">0.1</span><span class="op">+</span> combined_handwritten_for_psbc</span></code></pre></div>
<p>Now we are in better shape: if you look for minimum and maximum of the matrix <span class="math inline">\(combined\_handwritten\_for\_psbc\)</span> you will get <span class="math inline">\(0.5\)</span> and <span class="math inline">\(0.7000000000000001\)</span>.</p>
<!-- ```python -->
<!-- combined_handwritten_for_psbc.shape -->
<!-- ``` -->
<!-- $>>>$    (784, 6) -->
<!-- ```python -->
<!-- fig, ax = plt.subplots(1,6,figsize = (15,5)) -->
<!-- for i in range(6): -->
<!--     ax[i].imshow(np.reshape(combined_handwritten_for_psbc[:,i],(28,28)), cmap = 'binary') -->
<!--     ax[i].axis("off") -->
<!-- ``` -->
<div class="figure">
<img src="figures/output_105_0.png" alt="" />
<p class="caption">Some of the author’s handwritten examples for this tutorial.</p>
</div>
<p>Now let’s see how well the PSBC does in predicting them (note that, for the subset “0” and “1” of the MNIST database we already know that all these model perform quite well, separating these two classes with accuracy about 94%).</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="sec-mnist.html#cb12-1"></a><span class="cf">for</span> name <span class="kw">in</span> [<span class="st">&quot;W1S-NS&quot;</span>, <span class="st">&quot;W1S-S&quot;</span>, <span class="st">&quot;WNtS-NS&quot;</span>, <span class="st">&quot;WNtS-S&quot;</span>,<span class="op">\</span></span>
<span id="cb12-2"><a href="sec-mnist.html#cb12-2"></a>            <span class="st">&quot;W1S-Nt2&quot;</span>, <span class="st">&quot;W1S-Nt4&quot;</span>, <span class="st">&quot;W1S-Nt8&quot;</span>,<span class="op">\</span></span>
<span id="cb12-3"><a href="sec-mnist.html#cb12-3"></a>             <span class="st">&quot;WNtS-Nt1&quot;</span>,<span class="st">&quot;WNtS-Nt2&quot;</span>, <span class="st">&quot;WNtS-Nt4&quot;</span>, <span class="st">&quot;WNtS-Nt8&quot;</span>,<span class="op">\</span></span>
<span id="cb12-4"><a href="sec-mnist.html#cb12-4"></a>             <span class="st">&quot;Per_W1S-Nt2&quot;</span>, <span class="st">&quot;Per_W1S-Nt4&quot;</span>, <span class="st">&quot;Per_W1S-Nt8&quot;</span>,<span class="op">\</span></span>
<span id="cb12-5"><a href="sec-mnist.html#cb12-5"></a>             <span class="st">&quot;Per_WNtS-Nt1&quot;</span>,<span class="st">&quot;Per_WNtS-Nt2&quot;</span>, <span class="st">&quot;Per_WNtS-Nt4&quot;</span>, <span class="st">&quot;Per_WNtS-Nt8&quot;</span>]:</span>
<span id="cb12-6"><a href="sec-mnist.html#cb12-6"></a>            </span>
<span id="cb12-7"><a href="sec-mnist.html#cb12-7"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&quot;Examples/&quot;</span><span class="op">+</span>name<span class="op">+</span><span class="st">&quot;/simulation1/Full_model_properties.p&quot;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> fp:</span>
<span id="cb12-8"><a href="sec-mnist.html#cb12-8"></a>        load_mnist <span class="op">=</span> pickle.load(fp)</span>
<span id="cb12-9"><a href="sec-mnist.html#cb12-9"></a>    </span>
<span id="cb12-10"><a href="sec-mnist.html#cb12-10"></a>    psbc_testing <span class="op">=</span> Binary_Phase_Separation()</span>
<span id="cb12-11"><a href="sec-mnist.html#cb12-11"></a>    prediction <span class="op">=</span> <span class="op">\</span></span>
<span id="cb12-12"><a href="sec-mnist.html#cb12-12"></a>    psbc_testing.predict(</span>
<span id="cb12-13"><a href="sec-mnist.html#cb12-13"></a>        combined_handwritten_for_psbc,  load_mnist[<span class="st">&quot;best_par_U_model&quot;</span>], load_mnist[<span class="st">&quot;best_par_P_model&quot;</span>],<span class="op">\</span></span>
<span id="cb12-14"><a href="sec-mnist.html#cb12-14"></a>        subordinate <span class="op">=</span> load_mnist[<span class="st">&quot;best_par_U_model&quot;</span>][<span class="st">&quot;subordinate&quot;</span>]</span>
<span id="cb12-15"><a href="sec-mnist.html#cb12-15"></a>    )</span>
<span id="cb12-16"><a href="sec-mnist.html#cb12-16"></a>    <span class="bu">print</span>(<span class="st">&quot;Model&quot;</span>, name, <span class="st">&quot; predicts&quot;</span>, np.squeeze(prediction), <span class="st">&quot;and correct is, [0 0 0 1 1 1]&quot;</span> )</span></code></pre></div>
<p><span class="math inline">\(&gt;&gt;&gt;\)</span></p>
<pre><code>Model W1S-NS  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model W1S-S  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model WNtS-NS  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model WNtS-S  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model W1S-Nt2  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model W1S-Nt4  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model W1S-Nt8  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model WNtS-Nt1  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model WNtS-Nt2  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model WNtS-Nt4  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model WNtS-Nt8  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model Per_W1S-Nt2  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model Per_W1S-Nt4  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model Per_W1S-Nt8  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model Per_WNtS-Nt1  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model Per_WNtS-Nt2  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model Per_WNtS-Nt4  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]
Model Per_WNtS-Nt8  predicts [0 0 0 0 1 1] and correct is, [0 0 0 1 1 1]</code></pre>
<p>It is getting all correct, except for the 4th picture (which is in fact the way that I usually write, with that huge horizontal “foot”).</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Mnist">
<p>Lecun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” In <em>Proceedings of the Ieee</em>, 2278–2324.</p>
</div>
<div id="ref-Bin_phase_data">
<p>Monteiro, Rafael. 2020a. “Data Repository for the Paper ‘Binary Classification as a Phase Separation Process’.” <em>Zenodo Repository</em>. <a href="https://dx.doi.org/10.5281/zenodo.4005131">https://dx.doi.org/10.5281/zenodo.4005131</a>; Zenodo. <a href="https://doi.org/10.5281/zenodo.4005131">https://doi.org/10.5281/zenodo.4005131</a>.</p>
</div>
<div id="ref-Bin_phase_github">
<p>Monteiro, Rafael. 2020b. “Source Code for the Paper ‘Binary Classification as a Phase Separation Process’.” <em>GitHub Repository</em>. <a href="https://github.com/rafael-a-monteiro-math/Binary_classification_phase_separation">https://github.com/rafael-a-monteiro-math/Binary_classification_phase_separation</a>; GitHub.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-1d.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-phase-separation-binary-classifier-where-to-read-more-about-it.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Website.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
